# Freq2Vec
```python
import seqlearner
seqlearner.Freq2Vec(sequences, word_length, window_size, emb_dim, loss, epochs)
```

Freq2Vec is an Embedding Method. This class is wrapper for Freq2Vec Embedding method to apply on a set of sequences. Child class of WordEmbedder.

You can make an instance of it with the following parameters:

- __sequences__: numpy ndarray, list, or DataFrame
   sequences of data like protein sequences
- __word_length__: integer
    The length of each word in sequences to be separated from each other.
- __window_size__: integer
    Size of window for counting the number of neighbors.
- __emb_dim__:: integer
    Number of embedding vector dimensions.
- __loss__: basestring
    The loss function is going to be used on training phase.
- __epochs__: integer
    Number of epochs for training the embedding.

## See also

Freq2Vec.freq2vec_maker

The actual optimized objective is the mean of the output array across all datapoints.

For a few examples of such functions, check out the [losses source](https://github.com/keras-team/keras/blob/master/keras/losses.py).

## Available loss functions

{{autogenerated}}

----

**Note**: when using the `categorical_crossentropy` loss, your targets should be in categorical format (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros except for a 1 at the index corresponding to the class of the sample). In order to convert *integer targets* into *categorical targets*, you can use the Keras utility `to_categorical`:

```python
from keras.utils.np_utils import to_categorical

categorical_labels = to_categorical(int_labels, num_classes=None)
```

